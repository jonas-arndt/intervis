{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "median-production",
   "metadata": {},
   "source": [
    "# Intervis data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "upset-collar",
   "metadata": {},
   "source": [
    "## Process content from spreadsheet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-scanning",
   "metadata": {},
   "source": [
    "Download and convert the named pages of the related google spreadsheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "regulated-pickup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote file ./links.json.\n",
      "Wrote file ./references.json.\n",
      "Wrote file ./texts.json.\n",
      "Wrote file ./glossary.json.\n",
      "Wrote file ./disclosure.json.\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "file_gids = {\n",
    "    'links': '186216843',\n",
    "    'references': '1115773066',\n",
    "    'texts': '0',\n",
    "    'glossary': '1127543685',\n",
    "    'disclosure': '575388282',\n",
    "}\n",
    "\n",
    "spreadsheet_url = 'https://docs.google.com/spreadsheets/d/e/2PACX-1vTy2ONiejYXptt3uRLSeRqV1CJbbpi_68cz4Yeg9ZAdCC6tBhwK4DPgnLp6AwRK3EbYiMA2rLIVo0Z7/pub?output=csv'\n",
    "\n",
    "for filename, gid in file_gids.items():\n",
    "    df = pd.read_csv('%s&gid=%s' % (spreadsheet_url, gid), delimiter=',').dropna(how='all').fillna('')\n",
    "    data = df.to_dict('records')\n",
    "    \n",
    "    filepath = './%s.json' % filename\n",
    "    with open(filepath, 'w') as outfile:\n",
    "        json.dump(data, outfile, sort_keys=False, indent=4)\n",
    "        print('Wrote file %s.' % filepath)\n",
    "\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complimentary-success",
   "metadata": {},
   "source": [
    "Convert downloaded google files into language files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "plastic-attention",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://docs.google.com/forms/d/e/1FAIpQLSd9kykOGH0PTMdTi5de8w3Oz17lMRH-IjS0JNmIGfgBNdBp5w/viewform?usp=sf_link\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.9/site-packages/bs4/__init__.py:417: MarkupResemblesLocatorWarning: \"https://docs.google.com/forms/d/e/1FAIpQLSeynkCObZU11Vxb9PIy6e58I-US1x4FR_bl7oeX3MpGxowZpA/viewform?usp=sf_link\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client like requests to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote file ../locales/de.js\n",
      "Wrote file ../locales/en.js\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "files = {\n",
    "    'German': '../locales/de.js',\n",
    "    'English': '../locales/en.js',\n",
    "}\n",
    "\n",
    "# Merge google files\n",
    "texts_data = pd.read_json('texts.json')\n",
    "disclosure_data = pd.read_json('disclosure.json')\n",
    "\n",
    "data = pd.concat([texts_data, disclosure_data], ignore_index=True)\n",
    "data = data.set_index('ID')\n",
    "\n",
    "# Process glossary data\n",
    "glossary = {}\n",
    "glossary_data = pd.read_json('glossary.json')\n",
    "for language in files:\n",
    "    language_data = glossary_data.loc[glossary_data.language == language].set_index('ID').drop('language', axis=1)\n",
    "    glossary[language] = language_data.to_dict('index')\n",
    "\n",
    "# Process text data\n",
    "for column in files:\n",
    "    data[column] = data[column].str.replace('\\n','<br>') # add <br>\n",
    "    \n",
    "    # add title tags\n",
    "    for index, item in data[column].iteritems():\n",
    "        #print(index, item, data[column][index])\n",
    "        \n",
    "        soup = BeautifulSoup(item, 'html.parser')\n",
    "        for tooltip in soup.find_all(class_=\"tooltip\"):\n",
    "            if tooltip.has_attr('ref') and tooltip['ref'] in glossary[column]:\n",
    "                glossary_entry = glossary[column][tooltip['ref']]\n",
    "                tooltip['title'] = glossary_entry['description']\n",
    "            \n",
    "            if tooltip.has_attr('ref') and not tooltip['ref'] in glossary[column]:\n",
    "                print('Warning: glossary entry not found (%s: %s)' % (column, tooltip['ref']))\n",
    "        \n",
    "        for tooltip in soup.find_all('a'):\n",
    "            tooltip['target'] = '_blank'\n",
    "            \n",
    "        data[column][index] = str(soup)\n",
    "\n",
    "# output language files\n",
    "\n",
    "for (column, filepath) in files.items():\n",
    "    entries = data[column].to_dict()\n",
    "    output = 'export default ' + json.dumps(entries, indent=2)\n",
    "    \n",
    "    with open(filepath, 'w') as file:\n",
    "        file.write(output)\n",
    "        file.close()\n",
    "    \n",
    "    print('Wrote file %s' % filepath)\n",
    "\n",
    "# Done\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "selected-rendering",
   "metadata": {},
   "source": [
    "Add structured version of links file (plain list to dict with list for each type):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "weekly-conservation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote file ./links_structured.json\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = {}\n",
    "type_key = 'Type'\n",
    "output_file = './links_structured.json'\n",
    "\n",
    "df = pd.read_json('./links.json')\n",
    "for link_type in df[type_key].unique():\n",
    "    data[link_type] = df.loc[df[type_key] == link_type].to_dict('records')\n",
    "\n",
    "with open(output_file, 'w') as file:\n",
    "    json.dump(data, file, indent=2)\n",
    "    print('Wrote file %s' % output_file)\n",
    "    file.close()\n",
    "    \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nuclear-capitol",
   "metadata": {},
   "source": [
    "## Process images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daily-harbor",
   "metadata": {},
   "source": [
    "Compress fallback images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "valuable-ideal",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, shutil\n",
    "from PIL import Image \n",
    "\n",
    "# settings\n",
    "fallback_image_dirname = './_raw/fallback'\n",
    "destination_dirname = '../assets/fallback'\n",
    "\n",
    "# helper functions\n",
    "def copy_file(src_fpath, dest_fpath):\n",
    "    os.makedirs(os.path.dirname(dest_fpath), exist_ok=True)\n",
    "    shutil.copy(src_fpath, dest_fpath)\n",
    "\n",
    "def compress_image(filepath):\n",
    "    image_file = pathlib.Path(filepath)\n",
    "    image = Image.open(image_file)\n",
    "    \n",
    "    dpi = 150, 150\n",
    "    size = round(image.size[0] / 3), round(image.size[1] / 3)\n",
    "    \n",
    "    image.thumbnail(size)\n",
    "    image.save(image_file, dpi=dpi)\n",
    "        \n",
    "# image compression\n",
    "for path, subdirs, files in os.walk(fallback_image_dirname):\n",
    "    for name in files:\n",
    "        if name == '.DS_Store':\n",
    "            continue\n",
    "        \n",
    "        # Define ouput path\n",
    "        source_path = pathlib.PurePath(path, name)\n",
    "        output_path = pathlib.PurePath(destination_dirname, source_path.relative_to(*source_path.parts[:2]))\n",
    "        \n",
    "        # Copy and compress image\n",
    "        copy_file(source_path, output_path)\n",
    "        compress_image(output_path)\n",
    "        \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-complexity",
   "metadata": {},
   "source": [
    "Compress disclosure figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bored-syntax",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, shutil\n",
    "from PIL import Image, ImageColor\n",
    "\n",
    "# settings\n",
    "disclosure_image_dirname = './_raw/disclosure'\n",
    "destination_dirname = '../assets/disclosure'\n",
    "\n",
    "# helper functions\n",
    "def copy_file(src_fpath, dest_fpath):\n",
    "    os.makedirs(os.path.dirname(dest_fpath), exist_ok=True)\n",
    "    shutil.copy(src_fpath, dest_fpath)\n",
    "\n",
    "def compress_image(filepath):\n",
    "    image_file = pathlib.Path(filepath)\n",
    "    image = Image.open(image_file)\n",
    "    \n",
    "    dpi = 150, 150\n",
    "    size = 1200, 1200\n",
    "    \n",
    "    image.thumbnail(size)\n",
    "    image.save(image_file, dpi=dpi)\n",
    "\n",
    "def convert_image(png_filepath, jpg_filepath, background_color):\n",
    "    image_file = pathlib.Path(png_filepath)\n",
    "    image = Image.open(image_file)\n",
    "    \n",
    "    # add background by pasting it to another image\n",
    "    new_image = Image.new(\"RGBA\", image.size, ImageColor.getrgb(background_color))\n",
    "    new_image.paste(image, (0, 0), image)\n",
    "    \n",
    "    new_image.convert('RGB').save(jpg_filepath, quality=95, optimize=True, progressive=True)\n",
    "        \n",
    "# image compression\n",
    "for path, subdirs, files in os.walk(disclosure_image_dirname):\n",
    "    for name in files:\n",
    "        if name == '.DS_Store':\n",
    "            continue\n",
    "        \n",
    "        # Define ouput path\n",
    "        source_path = pathlib.PurePath(path, name)\n",
    "        png_output_path = pathlib.PurePath(destination_dirname, source_path.relative_to(*source_path.parts[:2]))\n",
    "        \n",
    "        # Copy and compress image\n",
    "        copy_file(source_path, png_output_path)\n",
    "        compress_image(png_output_path)\n",
    "        \n",
    "        # Convert image to jpeg\n",
    "        parentdir, filename = os.path.split(png_output_path)\n",
    "        filestem, fileextension = os.path.splitext(filename)\n",
    "\n",
    "        jpg_output_path = os.path.join(parentdir, filestem + '.jpg')\n",
    "        convert_image(png_output_path, jpg_output_path, '#F9FAFF')\n",
    "        \n",
    "        os.remove(png_output_path)\n",
    "        \n",
    "print('Done.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "affected-flour",
   "metadata": {},
   "source": [
    "Compress grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "impressive-amendment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, shutil\n",
    "from PIL import Image, ImageColor\n",
    "\n",
    "# settings\n",
    "grid_image_dirname = './_raw/grid'\n",
    "destination_dirname = '../assets/grid'\n",
    "\n",
    "# helper functions\n",
    "def copy_file(src_fpath, dest_fpath):\n",
    "    os.makedirs(os.path.dirname(dest_fpath), exist_ok=True)\n",
    "    shutil.copy(src_fpath, dest_fpath)\n",
    "\n",
    "def compress_image(filepath):\n",
    "    image_file = pathlib.Path(filepath)\n",
    "    image = Image.open(image_file)\n",
    "    \n",
    "    dpi = 150, 150\n",
    "    size = round(image.size[0] / 2), round(image.size[1] / 2)\n",
    "    \n",
    "    image.thumbnail(size)\n",
    "    image.save(image_file, dpi=dpi)\n",
    "\n",
    "def convert_image(png_filepath, jpg_filepath, background_color):\n",
    "    image_file = pathlib.Path(png_filepath)\n",
    "    image = Image.open(image_file)\n",
    "    \n",
    "    # add background by pasting it to another image\n",
    "    new_image = Image.new(\"RGBA\", image.size, ImageColor.getrgb(background_color))\n",
    "    new_image.paste(image, (0, 0), image)\n",
    "    \n",
    "    new_image.convert('RGB').save(jpg_filepath, quality=95, optimize=True, progressive=True)\n",
    "\n",
    "# image compression\n",
    "for path, subdirs, files in os.walk(grid_image_dirname):\n",
    "    for name in files:\n",
    "        if name == '.DS_Store':\n",
    "            continue\n",
    "        \n",
    "        # Define ouput path and copy file\n",
    "        source_path = pathlib.PurePath(path, name)\n",
    "        png_output_path = pathlib.PurePath(destination_dirname, source_path.relative_to(*source_path.parts[:2]))\n",
    "        copy_file(source_path, png_output_path)\n",
    "        \n",
    "        # Compress image\n",
    "        # compress_image(png_output_path)\n",
    "        \n",
    "        # Convert image to jpeg\n",
    "        jpg_conversions = [\n",
    "            'chapter3_frauenmitbehinderung.png',\n",
    "            'chapter3_frauenmithijab.png',\n",
    "            'chapter3_sintiundroma.png',\n",
    "            'dark_gray.png'\n",
    "        ]\n",
    "        if name in jpg_conversions:\n",
    "            parentdir, filename = os.path.split(png_output_path)\n",
    "            filestem, fileextension = os.path.splitext(filename)\n",
    "            jpg_output_path = os.path.join(parentdir, filestem + '.jpg')\n",
    "            \n",
    "            convert_image(png_output_path, jpg_output_path, '#F9FAFF')\n",
    "            os.remove(png_output_path)\n",
    "        \n",
    "print('Done.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
